# research_ideas
没空做的IDEA，放在这躺一躺
## 宽视野的图神经网络
GCN有一个非常坏的特性，就是视野的局限性，一个节点要与多跳外的节点产生信息交互，就必须经过多次GCN计算。当然CNN的计算核都是这副德行，GCN也免不了俗。

有没有具备全局视野，在图网络的问题里，也可以是具备所有可达节点视角的信息交互网络呢？

当然可以！一种解决思路是直接把attention搬到图网络里（不是GAT的那个，那玩意儿，难评），但是attention对所有节点都是一视同仁的，没有考虑到图上最关键的拓扑信息。另一种思路就是将远跳节点的信息按距离衰减再与当前节点进行信息交互，当然为了数据分布的稳定性，所有可达节点衰减相加后的权重需要乘以一个平衡系数，最好是个好算的平衡参数。

那么有没有这种衰减数列呢？还真有，而且好算，那就是矩阵指数。n跳邻居在邻接矩阵的表达式上就是`A^n`，可达节点可以通过`A^1+A^2+...+A^n`算出，而矩阵指数的定义如下：

![image](https://github.com/user-attachments/assets/35b29934-e8ea-44e6-8e96-e3364536ac2b)

因此，将`GCN=σ(D(A+I)D^-1HW)`改成`σ(D(exp(A)+I)D^-1HW)`,整个图神经网络就具备全局视野与局部聚焦的特性啦！而写exp(A)是一个常值，在网络拓扑固定之时就可以被计算出，因此并不会影响到网络的前向后向计算效率。

这个idea是22年想到的，应该会很快被人发成实验叭。

## 神经网络训练优化
人类有两种学习方式，短期学习和长期学习。反复的短期学习会逐渐固化为长期学习。包含记忆的神经网络是否可以通过长短期记忆学习的方式减轻训练负担呢？

当前的记忆神经元前向流程可以抽象为`σ(M(H+X)+B)=H'`，其中σ是激活函数，M和B是模型参数，H是代表记忆的状态向量，X是输入向量。显然M和B代表的是长期学习，而H是短期学习，问题在于H可以固化到M和B中吗？

结论是天然的，在传统的训练过程中，每一个H都首先被计算出来，而后通过δH/δM和δH/δB训练模型。问题在于，并不是所有的H都需要被固化到M和B中，人类大部分的短期状态都会被遗忘掉，而不是平等的固化为长期习惯或知识。换算到神经网络训练中，真的每一个token对应的H都值得被计算loss、梯度和训练吗？

反向计算梯度的耗时远大于前向计算，如果可以筛选出那些有用的H，有用的短时记忆，再只使用他们或者更大权重地使用他们训练模型，是不是可以缩减梯度计算的次数呢？

有用的H计算是简单的，对于一个文章，划分为句子，每一个句号结束的H都是一个阅读阶段的结束，他们的重要性大于句子中央的H，对于所有句号处的H，我们可以使用数据统计的方式、聚类的方式，计算出那些最常见的H，这些最常见的H就是最频繁出现的短时记忆，只用这些H训练模型就好了。其目的就在于，让模型更加容易地进入这些状态H，从人的角度类比，就是让人更快回想起当初的状态。
