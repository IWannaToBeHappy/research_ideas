# research_ideas
没空做的IDEA，放在这躺一躺
## 宽视野的图神经网络
GCN有一个非常坏的特性，就是视野的局限性，一个节点要与多跳外的节点产生信息交互，就必须经过多次GCN计算。当然CNN的计算核都是这副德行，GCN也免不了俗。

有没有具备全局视野，在图网络的问题里，也可以是具备所有可达节点视角的信息交互网络呢？

当然可以！一种解决思路是直接把attention搬到图网络里（不是GAT的那个，那玩意儿，难评），但是attention对所有节点都是一视同仁的，没有考虑到图上最关键的拓扑信息。另一种思路就是将远跳节点的信息按距离衰减再与当前节点进行信息交互，当然为了数据分布的稳定性，所有可达节点衰减相加后的权重需要乘以一个平衡系数，最好是个好算的平衡参数。

那么有没有这种衰减数列呢？还真有，而且好算，那就是矩阵指数。n跳邻居在邻接矩阵的表达式上就是`A^n`，可达节点可以通过`A^1+A^2+...+A^n`算出，而矩阵指数的定义如下：

![image](https://github.com/user-attachments/assets/35b29934-e8ea-44e6-8e96-e3364536ac2b)

因此，将`GCN=σ(D(A+I)D^-1HW)`改成`σ(D(exp(A)+I)D^-1HW)`,整个图神经网络就具备全局视野与局部聚焦的特性啦！而写exp(A)是一个常值，在网络拓扑固定之时就可以被计算出，因此并不会影响到网络的前向后向计算效率。

这个idea是22年想到的，应该会很快被人发成实验叭。

## 神经网络训练框架还不够人类

### 第一次记录 多次见训练样本强化
人类有两种学习方式，短期学习和长期学习。反复的短期学习会逐渐固化为长期学习。包含记忆的神经网络是否可以通过长短期记忆学习的方式减轻训练负担呢？

当前的记忆神经元前向流程可以抽象为`σ(M(H+X)+B)=H'`，其中σ是激活函数，M和B是模型参数，H是代表记忆的状态向量，X是输入向量。显然M和B代表的是长期学习，而H是短期学习，问题在于H可以固化到M和B中吗？

结论是天然的，在传统的训练过程中，每一个H都首先被计算出来，而后通过δH/δM和δH/δB训练模型。问题在于，并不是所有的H都需要被固化到M和B中，人类大部分的短期状态都会被遗忘掉，而不是平等的固化为长期习惯或知识。换算到神经网络训练中，真的每一个token对应的H都值得被计算loss、梯度和训练吗？

反向计算梯度的耗时远大于前向计算，如果可以筛选出那些有用的H，有用的短时记忆，再只使用他们或者更大权重地使用他们训练模型，是不是可以缩减梯度计算的次数呢？

有用的H计算是简单的，对于一个文章，划分为句子，每一个句号结束的H都是一个阅读阶段的结束，他们的重要性大于句子中央的H，对于所有句号处的H，我们可以使用数据统计的方式、聚类的方式，计算出那些最常见的H，这些最常见的H就是最频繁出现的短时记忆，只用这些H训练模型就好了。其目的就在于，让模型更加容易地进入这些状态H，从人的角度类比，就是让人更快回想起当初的状态。

### 第二次记录 推理时训练
与第一次记录不同，我重新推演了短期学习与长期学习能对模型训练带来的启示。这一次我想到了满足推理时训练的好方法。

先说说为什么要进行推理时训练。现在的模型始终遵循着数据集构建-训练-推理这一范式，在这个范式下，数据集构建与推理是完全割裂的，模型无法通过“使用”自主进化，当你发现模型的错误并在多轮对话中指出时，模型可能意识到他的错误并给出正确答案，但是一旦你与模型开启一段新的对话，它会毫不犹豫地犯下同样的错误，除非这一错误被数据集构建者意识到并进行针对性训练。这也就意味着，模型的上限取决于数据集构建者的技巧，而非海量的测试与使用。

如果我们将推理本身转化为模型训练的一部分，消除数据集构建与推理之间的隔阂，模型也就能越用越聪明，并真正为“真实世界模型”打下基础。作为真实世界模型，与环境交互并自主升级应该是基础能力，而非如现在的智能驾驶，机器人模型这般固化。

然而，推理时训练绝对不是将用户交互直接用于模型SFT，一方面算力负担不起，另一方面这也不符合我所说的长短期学习设想。要完成推理时训练，必须满足两个条件：1.训练消耗要小于等于推理消耗，2.训练应以真实环境的交互为奖励。

先说第一点，训练消耗小于等于推理消耗是完全可行的，从全参到lora就是降低训练消耗的一个旧有范例，但是lora训练的算力需求仍然太高了，更低的训练需求可以针对embed层，包括position_embed、system、还有动态prompt的叠加这些单向量展开，如果我们只对这些单向量进行训练，那么我们只需要提前计算好这些向量的计算图，其梯度更新消耗将降低至可以忽略不计的程度。当然，只对这些单向量进行训练完全不足以实现模型"自主优化"的最终目标，因此，推理时训练除了在推理过程中训练embed层向量，还需要在模型不被使用的时候进行“睡觉”，而睡觉的内容，就是embed层单向量的初始化和将embed层学到的loss传递到整个模型的所有参数。

也就是，先利用环境奖励在线训练小批量参数，再在离线阶段利用小批量参数训练整个模型。利用模型的部分参数，训练其他参数，这可能吗？

对模型M反复采样获取分布H(Y|a,b,X)，其中a为小参数，b为大参数，在修改a，b的同时保证分布不变，有

$db = - \frac{ \partial h/\partial a }{ \partial h/\partial b } da$

这同样可以通过梯度下降算法进行训练，而采用的X可以从积累的推理数据中提取（回忆），也可以从整个采样空间随机采样（举一反三）。

第二点，以真实环境的交互为奖励，其实就是将下一轮次的user内容直接作为loss的来源，也即我预期我的话语会得到什么样的回答。那么既然是真实世界，模型的输出就不应该单纯针对于user，还需要通过类似function_call等形式与现实进行交互，如预测现实世界的下一帧画面、function_call的预期回答等等，而非仅限于人机问答交互。

## 恶意软件基因
软件基因这个概念，是个为了创新而创新的生造词。这就导致很多人在说软件基因，但没有人知道软件基因到底是什么。从实用的角度出发，搞基因就是为了溯源，这个溯源不是说直接从某某字符串某某特征直接溯源到某一组织某一攻击者身上，而应该是溯源到父源样本。也就是说，搞基因真正有出路的，还是提取样本的一大堆特征，然后向前去搜索与他高重合度的旧样本，至于所谓的组织者归因，还是要通过父源样本的归类来进行推测。那么问题来了，这和以前的样本家族分类有什么区别？

区别就是对杂交的识别，家族分类在人类基因组里面也是一个很常见的课题，样本基因和样本家族不应该是两个独立的课题，从文字意义上来说就不应该，检测基因是检测家族的泛化。不同的地方在于，家族分类我们说这个样本属于什么家族，置信度多少多少，而软件基因溯源我们可以说这个样本与哪些家族一脉相承，他的这一片段与哪个家族高度重合，那个片段与哪个家族高度重合，这些高度重合的片段在整个恶意软件生态中是否有特异性，是否是某一类家族独有的，还是每个恶意样本都会这么做的。

我觉得这才是做恶意软件基因的意义。

## 大模型反汇编
当我们需要大模型做推理问题时，子任务拆分+reward构成了思维链的关键要素。通常我们的做法是在提问与微调回答中构建prompt让大模型遵照该子任务拆分进行回答，而后通过reward反馈大模型（进一步提问或微调训练）让其改进答案。问题在于，在反汇编领域子任务该如何拆分？

我们来看千问的回答：
```
Q：如果我要给你一段汇编代码，要求你将他翻译成伪C代码。你会按照什么思维链进行思考？
A：
1. 阅读并理解汇编代码
识别功能：首先，尝试理解这段汇编代码的整体功能是什么。这可能涉及到查看函数入口点、调用惯例、参数传递方式等。
识别数据结构：了解代码中使用的数据结构，比如数组、结构体、指针等。
理解控制流：识别代码中的条件分支、循环等控制结构。
2. 分析汇编指令
寄存器映射：确定哪些寄存器用于存储变量、地址或是临时值。
内存操作：分析加载（load）、存储（store）指令，理解它们如何与内存中的数据交互。
算术逻辑操作：识别加法、减法、位操作等指令，并考虑它们在C语言中的对应表达。
控制转移：理解跳转（jump）、调用（call）、返回（return）等指令的作用。
3. 构建C语言结构
变量声明：基于汇编代码中使用的寄存器和内存位置，决定在C语言中如何声明这些变量。
函数定义：根据汇编代码中的函数调用和返回点，定义相应的C函数。
逻辑重构：将汇编代码的逻辑重构成C语言中的if语句、while循环等。
4. 编写伪C代码
逐行翻译：将每一行汇编代码翻译成等效的C语言表达式或语句。
优化表达：利用C语言的高级特性简化代码，提高可读性。
注释解释：为复杂的转换添加注释，说明原始汇编代码的行为和翻译的理由。
5. 测试和验证
功能验证：确保翻译后的C代码实现了与原始汇编代码相同的功能。
边界条件检查：测试各种边界条件，确保C代码的健壮性。
性能对比：如果有必要，可以比较汇编代码和C代码在性能上的差异，特别是在循环和递归等密集计算场景下。
```

整理一下，反汇编的核心要素在于三点：数据结构识别、代码结构识别、一致性校验。
我们同样可以模拟传统反汇编器的操作原理，首先对每条汇编语句进行逐行或逐块翻译，而后通过代码优化技术不断使其贴近伪C代码。
实践中大模型在回答反汇编问题中可以返回伪C代码，但是总是存在遗漏、函数名修改等问题，逐行翻译可能是解决这一问题的有效途径。

## 大模型赋能搜索与广告的隐患
现在搜索工作都交给大模型搜索了，一大原因就是直接搜索的效率太低，且易受到广告的干扰，相较而言我们似乎更相信大模型的总结回答，甚至我们会期待大模型直接基于他搜索到的网页进行决策建议。

然而大模型搜索引擎同样可以接入广告，而且其广告成分更隐蔽、更难以察觉。

在使用deepseek联网搜索的过程中能够明确感受到搜索结果对于回答的影响，大模型对于搜索结果的信任度是极高的，这意味着一旦广告算法进入背后的搜索引擎，大模型会极度信任广告的宣传并主动以貌似客观的语言对用户进行决策建议。而因为鲜有用户真正打开搜索到的数十个搜索结果进行鉴别查看，这种决策建议比起以往的广告搜索更可能被用户所接受。这种商业模式似乎还未出现，但对我来说已有预见。

另一方面，即便没有广告算法的参杂，虚假搜索与类似回声谷的效应也不容忽视，我就见过以论坛内容作为回答依据，甚至论坛内容本身就是发帖者与大模型进行问答的记录分享的情况，这种不可信的搜索结果显然是不值得大模型进行完全信任地总结回答的。因此事实审查也将是未来大模型+搜索必不可少的一环，我们不能让大模型完全信任搜索结果从而误导用户，而要让大模型学会质疑。

## 大模型重复回答的原理与解决构思
现在开源出的大模型都存在对特定问题回答到一定长度后就可能陷入重复回答的情况，比如不断重复说一个字符、一段话或者不断简单的形式排比直到长度上限。

现在的解决方法是添加重复惩罚，即在词表映射的时候根据过往生成内功修改softmax分布结果，降低已出现过结果的概率。但是从decoder的层面看，模型仍旧可能在不断输出相似的向量，从而导致相似的概率分布。我们将整个decoder-only建模为`x_n = f(g(x_before)*x_n-1)`，其中g代表了包含注意力机制在内的对整体序列的处理，而f代表了包括前馈网络、softmax在内的简单转换。对于x_n不断重复的原因，我们可以归结为两种情况，一是因序列长度过大而导致函数g输出收敛，二是函数f对于x_n所在位置陷入类似`f(x)=x`的不动点。

从原理入手解决输出重复的问题，也可以从这两个方面进行探索，对于序列长度过大导致函数g输出收敛，现在的稀疏分组qkv应当可以从一定程度解决这一问题，通过某种机制引入相对位置向量也可行。但更重要的是避免函数f陷入或逼近不动点。

我们暂不考虑模拟退火这类需要不断迭代跳出局部最优的算法，一个简单的避免不动点的机制是使`x_n = f(g(x_before)*x_n-1) + \alpha /  |x_n-1 - f(g(x_before)*x_n-1)|`，也即使x_n与x_n-1之间存在一个随距离变进而增大的排斥量。这在代码上易于实现，效率也高，对于固定叠叠词的预期，则期望模型输出的概率分布克服这个排斥量，且叠词次数越多，越难以克服。

## 可信大模型反混淆辅助
将原本只有人能做但很繁琐的事情交给大模型处理，是大模型行业应用的合理任务产出。智能反混淆就是一个这样的例子，现在的反混淆技术还是高度依赖人工参与，虽然有Xyntia这种试图从符号执行方面生成一个最简逻辑程序的程序生成方法，但其输出显然对于反编译来说帮助甚微。当前已经出现了一些利用大模型分析软件、进行功能摘要、甚至恶意性判定的东西，但是其可信度与幻觉问题仍然是个很大的问题。从实验来看，大模型并不能处理过于复杂的逻辑，在代码处理上非常容易将输入样本向训练集中样本的理解靠拢，其表现为经常对部分代码忽略。

使用大模型进行反混淆，可以将反混淆任务拆解为最基础的简单任务，然后利用弱约束评估其正确性。针对控制流混淆，可以将反混淆任务拆解为谓词判断与控制流优化，其中谓词判断用于识别程序真实路径，也就是传统符号执行所进行的任务。不同与传统符号执行的是，大模型在程序语义上进行的谓词综合判断显然适用性更广，判断结果也不再局限于单一执行路径。弱化的简单谓词判断任务也比代码整体直接反混淆更易于大模型推理，如果我们将谓词判断在LLVM IR层展开，其谓词种类甚至只有少数的五六种可能，对于现行的大模型来说，这是十分容易推理的。

<img width="12931" height="3919" alt="fig1" src="https://github.com/user-attachments/assets/1bd053ca-8ae7-4efe-ad43-aa91480d51e3" />

## 大模型对偶自监督训练
恶意软件领域大模型的一大痛点就在于没有标签。要判断一个样本的TTP，没标签；要生成一个样本的功能摘要，没标签……

对于这类没标签任务，强化学习、自监督学习、伪监督学习通常是模型训练的最优解。然而困难在于，对于恶意软件功能摘要这类非常稀奇的新兴任务，甚至无法找到一个很好的reward函数对模型的生成摘要进行语义质量评估。恶意软件有很多，功能摘要很少，功能摘要的评估reward更少，这就是大模型在复杂专业领域的一大困境。所以，现在的强化学习更多只在数学、代码编写等易于验证的领域被使用、评估。

那有没有办法能够将reward函数从强化学习中去除或者说弱化呢？有的。

恶意软件功能摘要，或者说代码功能摘要是一类很特殊的任务。这类任务要求输入与输出的转换之间尽量不存在信息损失，与之相对应，常见的分类任务则不对信息损失过多计较。既然在完美的情况下，输入与输出之间不存在信息损失，那么就意味着我们可以使用一个相反的prompt要求模型针对输出生成输入，这一类根据不同任务要求可以将输入与输出对换位置的任务被我命名为对偶任务，其最广泛的例子就是语言翻译。

那么，评估此类任务的最优无标签方法就是利用reverse prompt要求模型根据自己的输出重新复现输入。比如正向prompt为`You are a helpful assistant that generates code summaries. Given a piece of code, provide a concise summary.`，对应的反向prompt就可以是`You are a helpful assistant that generates code. Given a code summary, generate the corresponding C code implementation.`

显而易见，如果不加其他约束，模型会倾向于将输入输出同质化，比如在摘要中塞入大量的代码原文。对于这种序列脱离其语言范畴的情况，我们就可以轻松地设计语言规范reward对其进行约束，而非在原始强化学习中绞尽脑汁去想验证输出语义正确性的reward，比如，对于中文目标环境，我们可以统计中文字符占completions的比重，对于英文目标环境，我们可以统计其输出英文的易读性。

不过这样训练会使得样本在梯度计算时翻倍，显存消耗也对应翻倍了，真是令人忧伤……
